"""
============================================================================
Flow Prefect : Pipeline ETL Complet (VERSION CORRIG√âE v3)
============================================================================
Propagation des tables trait√©es √† travers le pipeline :
- SFTP ‚Üí RAW d√©tecte les nouvelles tables
- RAW ‚Üí STAGING traite UNIQUEMENT ces tables
- STAGING ‚Üí ODS merge UNIQUEMENT ces tables
============================================================================
"""

from datetime import datetime
from prefect import flow
from prefect.logging import get_run_logger
import sys

sys.path.append(r'E:\Prefect\projects\ETL')

from flows.ingestion.db_metadata_import import db_metadata_import_flow
from flows.ingestion.sftp_to_raw import sftp_to_raw_flow
from flows.ingestion.raw_to_staging import raw_to_staging_flow
from flows.ingestion.staging_to_ods import staging_to_ods_flow
from flows.transformations.ods_to_prep import ods_to_prep_flow


@flow(name="[START] Pipeline ETL Complet v3 (Propagation)", log_prints=True)
def full_etl_pipeline(run_dbt: bool = False, import_metadata: bool = False):
    """
    Pipeline ETL complet avec propagation des tables trait√©es
    
    Args:
        run_dbt: Ex√©cuter dbt pour ODS ‚Üí PREP (d√©faut: False)
        import_metadata: Importer metadata Progress (d√©faut: False)
    
    Architecture :
    1. SFTP ‚Üí RAW : D√©tecte les nouvelles tables (ex: ['produit'])
    2. RAW ‚Üí STAGING : Traite UNIQUEMENT ces tables
    3. STAGING ‚Üí ODS : Merge UNIQUEMENT ces tables
    4. ODS ‚Üí PREP (dbt) : Optionnel
    """
    logger = get_run_logger()
    start_time = datetime.now()
    run_id = f"full_pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    logger.info("=" * 70)
    logger.info("[START] PIPELINE ETL COMPLET - VERSION 3.0 (PROPAGATION)")
    logger.info(f"üÜî Run ID: {run_id}")
    logger.info("=" * 70)
    
    results = {
        'run_id': run_id,
        'start_time': start_time.isoformat(),
        'metadata_imported': False,
        'raw_tables': 0,
        'staging_tables': 0,
        'ods_tables': 0,
        'dbt_models': 0,
        'errors': []
    }
    
    try:
        # ========================================
        # 1. METADATA PROGRESS (OPTIONNEL)
        # ========================================
        if import_metadata:
            logger.info("=" * 70)
            logger.info("üìö Phase 1 : Import metadata Progress")
            try:
                db_metadata_import_flow()
                results['metadata_imported'] = True
                logger.info("[OK] Metadata import√©s")
            except Exception as e:
                logger.warning(f"[WARN] Metadata skip : {e}")
                results['errors'].append(f"metadata: {str(e)}")
        else:
            logger.info("[SKIP] Phase 1 : Metadata ignor√©e (import_metadata=False)")
        
        # ========================================
        # 2. SFTP ‚Üí RAW (D√âTECTION)
        # ========================================
        logger.info("=" * 70)
        logger.info("üì• Phase 2 : SFTP ‚Üí RAW (Ingestion brute)")
        
        raw_result = sftp_to_raw_flow()
        results['raw_tables'] = raw_result['tables_loaded']
        results['raw_rows'] = raw_result.get('total_rows', 0)
        
        # [OK] R√âCUP√âRER LA LISTE DES TABLES TRAIT√âES
        tables_to_process = raw_result.get('tables', [])
        
        if raw_result['tables_loaded'] == 0:
            logger.info("[INFO] Aucune donn√©e SFTP √† traiter")
            logger.info("üõë Arr√™t du pipeline (rien √† faire)")
            results['end_time'] = datetime.now().isoformat()
            results['duration_seconds'] = (datetime.now() - start_time).total_seconds()
            return results
        
        logger.info(f"[OK] RAW : {results['raw_tables']} table(s) charg√©e(s)")
        logger.info(f"[LIST] Tables √† traiter : {tables_to_process}")
        
        # ========================================
        # 3. RAW ‚Üí STAGING (UNIQUEMENT LES NOUVELLES TABLES)
        # ========================================
        logger.info("=" * 70)
        logger.info("[LIST] Phase 3 : RAW ‚Üí STAGING_ETL (Hashdiff + Enrichissement)")
        logger.info(f"[TARGET] Traitement de {len(tables_to_process)} table(s) : {tables_to_process}")
        
        # [OK] PASSER LA LISTE DES TABLES √Ä TRAITER
        staging_result = raw_to_staging_flow(
            table_names=tables_to_process,  # ‚Üê NOUVELLE LOGIQUE
            run_id=run_id
        )
        results['staging_tables'] = staging_result['tables_processed']
        results['staging_rows'] = staging_result.get('total_rows', 0)
        
        logger.info(f"[OK] STAGING : {results['staging_tables']} table(s), {results['staging_rows']:,} lignes")
        
        # ========================================
        # 4. STAGING ‚Üí ODS (UNIQUEMENT LES NOUVELLES TABLES)
        # ========================================
        logger.info("=" * 70)
        logger.info("[SYNC] Phase 4 : STAGING_ETL ‚Üí ODS (Merge intelligent)")
        logger.info(f"[TARGET] Merge de {len(tables_to_process)} table(s) : {tables_to_process}")
        
        # [OK] PASSER LA LISTE DES TABLES √Ä MERGER
        ods_result = staging_to_ods_flow(
            table_names=tables_to_process,  # ‚Üê NOUVELLE LOGIQUE
            run_id=run_id,
            load_mode="AUTO"
        )
        results['ods_tables'] = ods_result['tables_merged']
        results['ods_rows_affected'] = ods_result.get('total_rows_affected', 0)
        
        logger.info(f"[OK] ODS : {results['ods_tables']} table(s), {results['ods_rows_affected']:,} lignes affect√©es")
        
        # ========================================
        # 5. ODS ‚Üí PREP (dbt) - OPTIONNEL
        # ========================================
        if run_dbt:
            logger.info("=" * 70)
            logger.info("[SETTINGS] Phase 5 : ODS ‚Üí PREP (dbt transformations)")
            
            try:
                dbt_result = ods_to_prep_flow(models="prep.*", run_tests=False)
                results['dbt_models'] = dbt_result.get('models_count', 0)
                results['dbt_tests_passed'] = dbt_result.get('tests_passed', None)
                
                logger.info(f"[OK] dbt : {results['dbt_models']} mod√®le(s)")
            except Exception as e:
                logger.error(f"[ERROR] Erreur dbt : {e}")
                results['errors'].append(f"dbt: {str(e)}")
        else:
            logger.info("[SKIP] Phase 5 : dbt ignor√©e (run_dbt=False)")
        
        # ========================================
        # R√âSUM√â FINAL
        # ========================================
        end_time = datetime.now()
        results['end_time'] = end_time.isoformat()
        results['duration_seconds'] = (end_time - start_time).total_seconds()
        results['success'] = len(results['errors']) == 0
        
        logger.info("=" * 70)
        logger.info("[OK] PIPELINE COMPLET TERMIN√â")
        logger.info("=" * 70)
        logger.info(f"[TIMER]  Dur√©e totale : {results['duration_seconds']:.2f}s")
        logger.info(f"üì• RAW : {results['raw_tables']} table(s), {results['raw_rows']:,} lignes")
        logger.info(f"[LIST] STAGING : {results['staging_tables']} table(s)")
        logger.info(f"[SYNC] ODS : {results['ods_tables']} table(s), {results['ods_rows_affected']:,} lignes affect√©es")
        
        if run_dbt:
            logger.info(f"[SETTINGS] dbt : {results['dbt_models']} mod√®le(s)")
        
        if results['errors']:
            logger.warning(f"[WARN] {len(results['errors'])} erreur(s) non bloquante(s)")
        
        logger.info("=" * 70)
        
        return results
        
    except Exception as e:
        logger.error(f"[ERROR] ERREUR CRITIQUE PIPELINE : {e}")
        results['end_time'] = datetime.now().isoformat()
        results['duration_seconds'] = (datetime.now() - start_time).total_seconds()
        results['success'] = False
        results['errors'].append(f"CRITICAL: {str(e)}")
        raise


@flow(name="üì• Pipeline Ingestion Python seul")
def ingestion_pipeline_only():
    """
    Pipeline ingestion Python uniquement (sans dbt)
    Avec propagation des tables d√©tect√©es
    """
    logger = get_run_logger()
    run_id = f"ingestion_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    logger.info("=" * 70)
    logger.info("üì• PIPELINE INGESTION : SFTP ‚Üí RAW ‚Üí STAGING ‚Üí ODS")
    logger.info(f"üÜî Run ID: {run_id}")
    logger.info("=" * 70)
    
    # 1. SFTP ‚Üí RAW (d√©tection)
    logger.info("üì• Phase 1 : SFTP ‚Üí RAW")
    raw_result = sftp_to_raw_flow()
    
    if raw_result['tables_loaded'] == 0:
        logger.info("[INFO] Aucune donn√©e √† traiter")
        return
    
    # [OK] R√©cup√©rer liste des tables charg√©es
    tables_to_process = raw_result.get('tables', [])
    logger.info(f"[LIST] Tables d√©tect√©es : {tables_to_process}")
    
    # 2. RAW ‚Üí STAGING (uniquement tables charg√©es)
    logger.info("=" * 70)
    logger.info("[LIST] Phase 2 : RAW ‚Üí STAGING_ETL")
    staging_result = raw_to_staging_flow(
        table_names=tables_to_process,
        run_id=run_id
    )
    
    # 3. STAGING ‚Üí ODS (uniquement tables charg√©es)
    logger.info("=" * 70)
    logger.info("[SYNC] Phase 3 : STAGING_ETL ‚Üí ODS")
    ods_result = staging_to_ods_flow(
        table_names=tables_to_process,
        run_id=run_id
    )
    
    logger.info("=" * 70)
    logger.info("[OK] PIPELINE INGESTION TERMIN√â")
    logger.info(f"[DATA] {len(tables_to_process)} table(s) trait√©e(s)")
    logger.info("=" * 70)


if __name__ == "__main__":
    import sys
    
    # python full_pipeline.py --ingestion-only
    if len(sys.argv) > 1 and sys.argv[1] == '--ingestion-only':
        ingestion_pipeline_only()
    # python full_pipeline.py --with-dbt
    elif len(sys.argv) > 1 and sys.argv[1] == '--with-dbt':
        full_etl_pipeline(run_dbt=True)
    # python full_pipeline.py (d√©faut: sans dbt)
    else:
        full_etl_pipeline(run_dbt=False)